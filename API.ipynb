%%writefile api.py

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import os
import uvicorn
import nest_asyncio
from pyngrok import ngrok

# Define the request body format
class LogRequest(BaseModel):
    log_data: str

# Create the FastAPI app instance
app = FastAPI()

# Global variables to hold the model and tokenizer
model = None
tokenizer = None

# Load the fine-tuned model when the app starts
@app.on_event("startup")
def load_model_and_tokenizer():
    global model, tokenizer
    try:
        # Load the base model
        base_model_name = "microsoft/phi-2"
        # 4-bit Quantization Configuration
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto"
        )
        # Load the fine-tuned adapter weights
        adapter_path = "./models/phi-finetuned"
        model = PeftModel.from_pretrained(base_model, adapter_path)

        # Load the tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, pad_token="<|endoftext|>")
        print("Model and tokenizer loaded successfully!")

    except Exception as e:
        print(f"Error loading model: {e}")
        raise HTTPException(status_code=500, detail="Model could not be loaded.")

# Define the API endpoint for log analysis
@app.post("/analyze")
def analyze_log_endpoint(request: LogRequest):
    if not model or not tokenizer:
        raise HTTPException(status_code=500, detail="Model is not loaded.")

    log_content = request.log_data

    # Create the prompt for the model
    prompt = (
        "### Instruction:\nAnalyze the following log and determine its sentiment. "
        "Classify it as one of the following: sadness, joy, love, anger, fear, or surprise. "
        "Explain why and provide an example of a similar log.\n\n"
        f"### Log Entry:\n{log_content}\n\n"
        "### Output:"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Generate the response from the model
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            top_p=0.95,
            temperature=0.7,
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Parse the output to get the structured response
    output_lines = generated_text.split("### Output:")[1].strip().split('\n')

    response = {
        "label": output_lines[0].split(':')[-1].strip() if len(output_lines) > 0 and ':' in output_lines[0] else "N/A",
        "explanation": output_lines[1].split(':')[-1].strip() if len(output_lines) > 1 and ':' in output_lines[1] else "N/A"
    }

    # Add a risk score based on the sentiment
    sentiment_to_score = {
        "sadness": 70, "anger": 90, "fear": 80,
        "joy": 10, "love": 20, "surprise": 50
    }
    response["risk_score"] = sentiment_to_score.get(response["label"].lower(), 30)

    return response

# Now run the API with uvicorn
ngrok_tunnel = ngrok.connect(8000)
print(f"Public URL for your API: {ngrok_tunnel.public_url}")
nest_asyncio.apply()
uvicorn.run(app, port=8000)
